{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用1层隐藏层的神经网络分类二维数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欢迎来到第3周的编程作业。 现在是时候建立你的第一个神经网络了，它将具有一层隐藏层。 你将看到此模型与你使用逻辑回归实现的模型之间的巨大差异。\n",
    "\n",
    "你将学到如何：\n",
    "\n",
    "实现具有单个隐藏层的2分类神经网络\n",
    "使用具有非线性激活函数的神经元，例如tanh\n",
    "计算交叉熵损失\n",
    "实现前向和后向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases import *\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1) # set a seed so that the results are consistent\n",
    "# np.random.seed()函数用于生成指定随机数。\n",
    "# seed()被设置了之后，np,random.random()可以按顺序产生一组固定的数组，\n",
    "# 如果使用相同的seed()值，则每次生成的随机数都相同，\n",
    "# 如果不设置这个值，那么每次生成的随机数不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_planar_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用matplotlib可视化数据集。 \n",
    "数据看起来像是带有一些红色（标签y = 0）和一些蓝色（y = 1）点的“花”。 \n",
    "我们的目标是建立一个适合该数据的分类模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data:\n",
    "plt.scatter(X[0, :], X[1, :], c=Y.reshape(X[0,:].shape), s=40, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在你有：\n",
    "   - 包含特征（x1，x2）的numpy数组（矩阵）X\n",
    "   - 包含标签（红色：0，蓝色：1）的numpy数组（向量）Y。\n",
    "\n",
    "首先让我们深入地了解一下我们的数据。\n",
    "\n",
    "练习：数据集中有多少个训练示例？ 另外，变量“ X”和“ Y”的“shape”是什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "shape_X = X.shape\n",
    "shape_Y = Y.shape\n",
    "\n",
    "m = shape_X[1]  # training set size\n",
    "### END CODE HERE ###\n",
    "\n",
    "print ('The shape of X is: ' + str(shape_X))\n",
    "print ('The shape of Y is: ' + str(shape_Y))\n",
    "print ('I have m = %d training examples!' % (m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- 简单Logistic回归\n",
    "在构建完整的神经网络之前，首先让我们看看逻辑回归在此问题上的表现。 你可以使用sklearn的内置函数来执行此操作。 运行以下代码以在数据集上训练逻辑回归分类器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.linear_model.LogisticRegressionCV()\n",
    "clf.fit(X.T, Y.T)\n",
    "# Plot the decision boundary for logistic regression\n",
    "plot_decision_boundary(lambda x: clf.predict(x), X, Y)\n",
    "plt.title(\"Logistic Regression\")\n",
    "\n",
    "# Print accuracy\n",
    "LR_predictions = clf.predict(X.T)\n",
    "print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n",
    "       '% ' + \"(percentage of correctly labelled datapoints)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于数据集不是线性可分类的，因此逻辑回归效果不佳。 让我们试试是否神经网络会做得更好吧!\n",
    "\n",
    "4- 神经网络模型\n",
    "从上面我们可以得知Logistic回归不适用于“flower数据集”。现在你将训练带有单个隐藏层的神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立神经网络的一般方法是：\n",
    "1.定义神经网络结构（输入单元数，隐藏单元数等）。\n",
    "2.初始化模型的参数\n",
    "3.循环：\n",
    "\n",
    "实现前向传播\n",
    "计算损失\n",
    "后向传播以获得梯度\n",
    "更新参数（梯度下降）\n",
    "我们通常会构建辅助函数来计算第1-3步，然后将它们合并为nn_model()函数。一旦构建了nn_model()并学习了正确的参数，就可以对新数据进行预测。\n",
    "\n",
    "\n",
    "4.1- 定义神经网络结构\n",
    "练习：定义三个变量：\n",
    "     - n_x：输入层的大小\n",
    "     - n_h：隐藏层的大小（将其设置为4）\n",
    "     - n_y：输出层的大小\n",
    "\n",
    "提示：使用shape来找到n_x和n_y。 另外，将隐藏层大小硬编码为4。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: layer_sizes\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    n_x = X.shape[0] # size of input layer, 2\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0] # size of output layer, 1\n",
    "    ### END CODE HERE ###\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, Y_assess = layer_sizes_test_case()\n",
    "# X: (5, 3)   Y: (2, 3)\n",
    "(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2- 初始化模型的参数\n",
    "练习：实现函数 initialize_parameters()。\n",
    "\n",
    "说明：\n",
    "\n",
    "请确保参数大小正确。 如果需要，也可参考上面的神经网络图。\n",
    "使用随机值初始化权重矩阵。\n",
    "     - 使用：np.random.randn（a，b）* 0.01随机初始化维度为（a，b）的矩阵。\n",
    "将偏差向量初始化为零。\n",
    "     - 使用：np.zeros((a,b)) 初始化维度为（a，b）零的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    ### END CODE HERE ###\n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x, n_h, n_y = initialize_parameters_test_case()\n",
    "\n",
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3- 循环\n",
    "问题：实现forward_propagation（）。\n",
    "\n",
    "说明：\n",
    "\n",
    "在上方查看分类器的数学表示形式。\n",
    "你可以使用内置在笔记本中的sigmoid()函数。\n",
    "你也可以使用numpy库中的np.tanh（）函数。\n",
    "必须执行以下步骤：\n",
    "     1.使用parameters [“ ..”]从字典“ parameters”（这是initialize_parameters（）的输出）中检索出每个参数。\n",
    "     2.实现正向传播，计算Zf1, Af1, Zf2和Af2（所有训练数据的预测结果向量）。\n",
    "向后传播所需的值存储在cache中,  cache将作为反向传播函数的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "\n",
    "    # dim_tuple = layer_sizes(X, Y)\n",
    "    # params = initialize_parameters(dim_tuple[0], dim_tuple[1], dim_tuple[2])\n",
    "    \n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # forward propagation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, parameters = forward_propagation_test_case()\n",
    "\n",
    "A2, cache = forward_propagation(X_assess, parameters)\n",
    "\n",
    "# Note: we use the mean here just to make sure that your output matches ours. \n",
    "print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "练习：实现compute_cost（）以计算损失的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "     ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    logprobs = Y * np.log(A2) + (1-Y) * np.log(1-A2)\n",
    "    cost = -1/m * np.sum(logprobs)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.6929198937761265\n"
     ]
    }
   ],
   "source": [
    "A2, Y_assess, parameters = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(A2, Y_assess, parameters)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8da01e5a71448ea74f54d88afa8911010d1d12e23bc7e103d40d5def4a09152c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
